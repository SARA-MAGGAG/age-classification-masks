{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installation des librairies nÃ©cessaires (sans tensorflow-addons)\n",
        "!pip install -q tensorflow scikit-plot\n",
        "\n",
        "# Importations\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
        "\n",
        "# Configuration\n",
        "BASE_PATH = \"/content/drive/MyDrive/MobileNetV2/data\"\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "SEED = 42\n",
        "\n",
        "# Fixer les seeds pour la reproductibilitÃ©\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# 1. ANALYSE DES DONNÃ‰ES\n",
        "print(\"=== ANALYSE DU DÃ‰SÃ‰QUILIBRE ===\")\n",
        "\n",
        "# Lister toutes les images et leurs classes\n",
        "image_paths = []\n",
        "labels = []\n",
        "class_names = []\n",
        "\n",
        "for class_dir in os.listdir(BASE_PATH):\n",
        "    class_path = os.path.join(BASE_PATH, class_dir)\n",
        "    if os.path.isdir(class_path):\n",
        "        images_in_class = []\n",
        "        for img_file in os.listdir(class_path):\n",
        "            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                full_path = os.path.join(class_path, img_file)\n",
        "                image_paths.append(full_path)\n",
        "                labels.append(class_dir)\n",
        "                class_names.append(class_dir)\n",
        "                images_in_class.append(full_path)\n",
        "        print(f\"Dossier {class_dir}: {len(images_in_class)} images\")\n",
        "\n",
        "class_names = sorted(list(set(class_names)))\n",
        "print(f\"\\nClasses trouvÃ©es: {class_names}\")\n",
        "print(f\"Nombre total d'images: {len(image_paths)}\")\n",
        "\n",
        "# Compter les images par classe\n",
        "class_counts = Counter(labels)\n",
        "print(\"\\nDistribution rÃ©elle des classes:\")\n",
        "for class_name in class_names:\n",
        "    count = class_counts[class_name]\n",
        "    print(f\"{class_name}: {count} images ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(class_counts.keys(), [class_counts[cn] for cn in class_names])\n",
        "plt.title('Distribution rÃ©elle des classes')\n",
        "plt.xlabel('Classe d\\'Ã¢ge')\n",
        "plt.ylabel('Nombre d\\'images')\n",
        "for bar, count in zip(bars, [class_counts[cn] for cn in class_names]):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
        "             str(count), ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. STRATÃ‰GIE D'Ã‰QUILIBRAGE\n",
        "print(\"\\n=== STRATÃ‰GIE D'Ã‰QUILIBRAGE ===\")\n",
        "\n",
        "# Cibles pour chaque classe (AVANT division)\n",
        "target_counts = {\n",
        "    '1-20': 6638,    # Garder telles quelles\n",
        "    '21-50': 8776,   # SOUS-Ã©chantillonnage (rÃ©duire de 21497 Ã  8776)\n",
        "    '51-100': 6638   # SUR-Ã©chantillonnage (augmenter Ã  6638)\n",
        "}\n",
        "\n",
        "print(\"Cibles par classe (avant division train/val/test):\")\n",
        "for class_name in class_names:\n",
        "    original = class_counts[class_name]\n",
        "    target = target_counts[class_name]\n",
        "    if target > original:\n",
        "        action = \"SUR-Ã©chantillonnage\"\n",
        "    elif target < original:\n",
        "        action = \"SOUS-Ã©chantillonnage\"\n",
        "    else:\n",
        "        action = \"Garder telles quelles\"\n",
        "    print(f\"{class_name}: {original} â†’ {target} images ({action})\")\n",
        "\n",
        "# 3. PRÃ‰PARATION DES DONNÃ‰ES AVEC Ã‰QUILIBRAGE PUIS DIVISION GLOBALE (70/15/15)\n",
        "def prepare_datasets_70_15_15():\n",
        "    \"\"\"PrÃ©pare les datasets avec Ã©quilibrage d'abord, puis division globale 70/15/15\"\"\"\n",
        "\n",
        "    # Collecter tous les chemins par classe\n",
        "    class_paths = {}\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(BASE_PATH, class_name)\n",
        "        images = []\n",
        "        for img_file in os.listdir(class_dir):\n",
        "            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                images.append(os.path.join(class_dir, img_file))\n",
        "        class_paths[class_name] = images\n",
        "\n",
        "    print(f\"\\nImages disponibles par classe:\")\n",
        "    for class_name in class_names:\n",
        "        print(f\"{class_name}: {len(class_paths[class_name])} images\")\n",
        "\n",
        "    # Ã‰TAPE 1: Appliquer l'Ã©quilibrage pour chaque classe\n",
        "    balanced_paths_by_class = {}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        paths = class_paths[class_name]\n",
        "        target = target_counts[class_name]\n",
        "        original_count = len(paths)\n",
        "\n",
        "        if original_count > target:\n",
        "            # SOUS-Ã©chantillonnage\n",
        "            selected_paths = random.sample(paths, target)\n",
        "            print(f\"{class_name}: {original_count} â†’ {len(selected_paths)} (SOUS-Ã©chantillonnage)\")\n",
        "\n",
        "        elif original_count < target:\n",
        "            # SUR-Ã©chantillonnage\n",
        "            needed = target - original_count\n",
        "            extra_samples = random.choices(paths, k=needed)\n",
        "            selected_paths = paths + extra_samples\n",
        "            print(f\"{class_name}: {original_count} â†’ {len(selected_paths)} (SUR-Ã©chantillonnage)\")\n",
        "\n",
        "        else:\n",
        "            # DÃ©jÃ  Ã  la bonne taille\n",
        "            selected_paths = paths\n",
        "            print(f\"{class_name}: {original_count} (gardÃ©es telles quelles)\")\n",
        "\n",
        "        balanced_paths_by_class[class_name] = selected_paths\n",
        "\n",
        "    # Ã‰TAPE 2: Division globale (70% train, 15% val, 15% test)\n",
        "    print(f\"\\nDivision globale (70% train, 15% val, 15% test):\")\n",
        "\n",
        "    all_train_paths = []\n",
        "    all_train_labels = []\n",
        "    all_val_paths = []\n",
        "    all_val_labels = []\n",
        "    all_test_paths = []\n",
        "    all_test_labels = []\n",
        "\n",
        "    for class_name in class_names:\n",
        "        paths = balanced_paths_by_class[class_name]\n",
        "        labels_list = [class_name] * len(paths)\n",
        "\n",
        "        # Division: 70% train, 30% temporaire (val+test)\n",
        "        train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "            paths, labels_list, test_size=0.3, random_state=SEED, stratify=labels_list\n",
        "        )\n",
        "\n",
        "        # Division du temporaire: 50% val (15% du total), 50% test (15% du total)\n",
        "        val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "            temp_paths, temp_labels, test_size=0.5, random_state=SEED, stratify=temp_labels\n",
        "        )\n",
        "\n",
        "        all_train_paths.extend(train_paths)\n",
        "        all_train_labels.extend(train_labels)\n",
        "        all_val_paths.extend(val_paths)\n",
        "        all_val_labels.extend(val_labels)\n",
        "        all_test_paths.extend(test_paths)\n",
        "        all_test_labels.extend(test_labels)\n",
        "\n",
        "        # Afficher les statistiques par classe\n",
        "        total = len(paths)\n",
        "        train_count = len(train_paths)\n",
        "        val_count = len(val_paths)\n",
        "        test_count = len(test_paths)\n",
        "\n",
        "        print(f\"\\n{class_name}:\")\n",
        "        print(f\"  Train: {train_count} images ({train_count/total*100:.1f}%)\")\n",
        "        print(f\"  Validation: {val_count} images ({val_count/total*100:.1f}%)\")\n",
        "        print(f\"  Test: {test_count} images ({test_count/total*100:.1f}%)\")\n",
        "        print(f\"  Total: {total} images\")\n",
        "\n",
        "    # CrÃ©er les DataFrames\n",
        "    train_df = pd.DataFrame({'filename': all_train_paths, 'class': all_train_labels})\n",
        "    val_df = pd.DataFrame({'filename': all_val_paths, 'class': all_val_labels})\n",
        "    test_df = pd.DataFrame({'filename': all_test_paths, 'class': all_test_labels})\n",
        "\n",
        "    print(f\"\\nTotaux globaux:\")\n",
        "    print(f\"  Train: {len(train_df)} images\")\n",
        "    print(f\"  Validation: {len(val_df)} images\")\n",
        "    print(f\"  Test: {len(test_df)} images\")\n",
        "    print(f\"  Total: {len(train_df) + len(val_df) + len(test_df)} images\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# PrÃ©parer les datasets avec division 70/15/15\n",
        "train_df, val_df, test_df = prepare_datasets_70_15_15()\n",
        "\n",
        "# 4. PRÃ‰PARATION DES GÃ‰NÃ‰RATEURS\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# CrÃ©er les gÃ©nÃ©rateurs\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"\\nGÃ©nÃ©rateurs crÃ©Ã©s:\")\n",
        "print(f\"  Train: {len(train_df)} images\")\n",
        "print(f\"  Validation: {len(val_df)} images\")\n",
        "print(f\"  Test: {len(test_df)} images\")\n",
        "\n",
        "# 5. CALCUL DES POIDS DES CLASSES (basÃ© sur le train)\n",
        "def compute_class_weights_for_loss(labels):\n",
        "    \"\"\"Calcule les poids des classes\"\"\"\n",
        "    unique_classes = np.unique(labels)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=unique_classes,\n",
        "        y=labels\n",
        "    )\n",
        "    class_weights = [float(w) for w in class_weights]\n",
        "    return dict(zip(range(len(unique_classes)), class_weights))\n",
        "\n",
        "# Calculer les poids sur le train seulement\n",
        "class_weights = compute_class_weights_for_loss(train_df['class'])\n",
        "print(f\"\\nPoids des classes calculÃ©s (sur train): {class_weights}\")\n",
        "\n",
        "# 6. FOCAL LOSS\n",
        "class FocalLoss(keras.losses.Loss):\n",
        "    \"\"\"Focal Loss pour dÃ©sÃ©quilibre de classes\"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=0.25, name='focal_loss'):\n",
        "        super().__init__(name=name)\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        weight = self.alpha * y_true * tf.pow((1 - y_pred), self.gamma)\n",
        "        focal_loss = weight * cross_entropy\n",
        "        focal_loss = tf.reduce_sum(focal_loss, axis=1)\n",
        "        return tf.reduce_mean(focal_loss)\n",
        "\n",
        "# 7. MODÃˆLE MobileNetV2\n",
        "def build_mobilenetv2_model(num_classes=3):\n",
        "    \"\"\"Construit le modÃ¨le\"\"\"\n",
        "\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "\n",
        "    # Fine-tuning\n",
        "    base_model.trainable = True\n",
        "    fine_tune_at = 100\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Architecture\n",
        "    inputs = keras.Input(shape=(224, 224, 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "# Construction\n",
        "model = build_mobilenetv2_model(num_classes=3)\n",
        "\n",
        "# 8. COMPILATION\n",
        "initial_learning_rate = 1e-4\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=lr_schedule,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999\n",
        ")\n",
        "\n",
        "# Utiliser Focal Loss\n",
        "loss_fn = FocalLoss(gamma=2.0, alpha=0.25)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=['accuracy', keras.metrics.Precision(name='precision'),\n",
        "             keras.metrics.Recall(name='recall')]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 9. CALLBACKS\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        '/content/drive/MyDrive/MobileNetV2/best_model.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        mode='min',\n",
        "        verbose=1\n",
        "    ),\n",
        "    CSVLogger(\n",
        "        '/content/drive/MyDrive/MobileNetV2/training_log.csv',\n",
        "        separator=',',\n",
        "        append=False\n",
        "    )\n",
        "]\n",
        "\n",
        "# 10. ENTRAÃŽNEMENT\n",
        "print(\"\\n=== DÃ‰BUT DE L'ENTRAÃŽNEMENT ===\")\n",
        "\n",
        "# Calcul des steps\n",
        "train_steps = len(train_df) // BATCH_SIZE\n",
        "if len(train_df) % BATCH_SIZE != 0:\n",
        "    train_steps += 1\n",
        "\n",
        "val_steps = len(val_df) // BATCH_SIZE\n",
        "if len(val_df) % BATCH_SIZE != 0:\n",
        "    val_steps += 1\n",
        "\n",
        "print(f\"Train steps par epoch: {train_steps}\")\n",
        "print(f\"Validation steps: {val_steps}\")\n",
        "print(f\"Train images: {len(train_df)}\")\n",
        "print(f\"Validation images: {len(val_df)}\")\n",
        "\n",
        "# EntraÃ®nement standard (sans application manuelle des poids)\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 11. VISUALISATION\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Train')\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation')\n",
        "axes[0, 0].set_title('Loss')\n",
        "axes[0, 0].set_xlabel('Epochs')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Train')\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Validation')\n",
        "axes[0, 1].set_title('Accuracy')\n",
        "axes[0, 1].set_xlabel('Epochs')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision\n",
        "axes[0, 2].plot(history.history['precision'], label='Train')\n",
        "axes[0, 2].plot(history.history['val_precision'], label='Validation')\n",
        "axes[0, 2].set_title('Precision')\n",
        "axes[0, 2].set_xlabel('Epochs')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Recall\n",
        "axes[1, 0].plot(history.history['recall'], label='Train')\n",
        "axes[1, 0].plot(history.history['val_recall'], label='Validation')\n",
        "axes[1, 0].set_title('Recall')\n",
        "axes[1, 0].set_xlabel('Epochs')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Comparaison Train vs Val Accuracy\n",
        "axes[1, 1].plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
        "axes[1, 1].plot(history.history['val_accuracy'], label='Val Accuracy', color='orange', linestyle='--')\n",
        "axes[1, 1].set_title('Train vs Validation Accuracy')\n",
        "axes[1, 1].set_xlabel('Epochs')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Comparaison Train vs Val Loss\n",
        "axes[1, 2].plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "axes[1, 2].plot(history.history['val_loss'], label='Val Loss', color='orange', linestyle='--')\n",
        "axes[1, 2].set_title('Train vs Validation Loss')\n",
        "axes[1, 2].set_xlabel('Epochs')\n",
        "axes[1, 2].set_ylabel('Loss')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 12. Ã‰VALUATION SUR LE TEST SET\n",
        "print(\"\\n=== Ã‰VALUATION SUR LE TEST SET ===\")\n",
        "\n",
        "# Charger le meilleur modÃ¨le\n",
        "best_model = keras.models.load_model('/content/drive/MyDrive/MobileNetV2/best_model.h5',\n",
        "                                    custom_objects={'FocalLoss': FocalLoss})\n",
        "\n",
        "# PrÃ©dictions sur le test set\n",
        "test_steps = len(test_df) // BATCH_SIZE\n",
        "if len(test_df) % BATCH_SIZE != 0:\n",
        "    test_steps += 1\n",
        "\n",
        "test_generator.reset()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_scores = []\n",
        "\n",
        "for step in range(test_steps):\n",
        "    x_batch, y_batch = test_generator.next()\n",
        "\n",
        "    # PrÃ©diction\n",
        "    batch_pred = best_model.predict(x_batch, verbose=0)\n",
        "\n",
        "    # Vraies classes\n",
        "    batch_true = np.argmax(y_batch, axis=1)\n",
        "    y_true.extend(batch_true)\n",
        "\n",
        "    # Classes prÃ©dites\n",
        "    batch_pred_classes = np.argmax(batch_pred, axis=1)\n",
        "    y_pred.extend(batch_pred_classes)\n",
        "\n",
        "    # Scores\n",
        "    y_scores.extend(batch_pred)\n",
        "\n",
        "# MÃ©triques\n",
        "test_accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "test_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1-Score (macro): {test_f1:.4f}\")\n",
        "\n",
        "# Rapport de classification\n",
        "class_indices = train_generator.class_indices\n",
        "reverse_indices = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "print(\"\\nRapport de classification dÃ©taillÃ©:\")\n",
        "print(classification_report(\n",
        "    y_true, y_pred,\n",
        "    target_names=[reverse_indices[i] for i in range(len(class_indices))],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[reverse_indices[i] for i in range(len(class_indices))],\n",
        "            yticklabels=[reverse_indices[i] for i in range(len(class_indices))])\n",
        "plt.title('Matrice de confusion - Test Set')\n",
        "plt.ylabel('Vraie classe')\n",
        "plt.xlabel('Classe prÃ©dite')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/MobileNetV2/confusion_matrix_test.png')\n",
        "plt.show()\n",
        "\n",
        "# 13. SAUVEGARDE\n",
        "print(\"\\n=== SAUVEGARDE ===\")\n",
        "best_model.save('/content/drive/MyDrive/MobileNetV2/final_age_classifier.h5')\n",
        "print(\"âœ“ ModÃ¨le final sauvegardÃ©\")\n",
        "\n",
        "# Sauvegarde des mÃ©tadonnÃ©es\n",
        "class_indices_df = pd.DataFrame(list(class_indices.items()),\n",
        "                                columns=['class_name', 'class_index'])\n",
        "class_indices_df.to_csv('/content/drive/MyDrive/MobileNetV2/class_indices.csv', index=False)\n",
        "print(\"âœ“ Indices sauvegardÃ©s\")\n",
        "\n",
        "# Poids des classes\n",
        "class_weights_df = pd.DataFrame(list(class_weights.items()),\n",
        "                               columns=['class_index', 'weight'])\n",
        "class_weights_df['class_name'] = [reverse_indices[i] for i in class_weights_df['class_index']]\n",
        "class_weights_df.to_csv('/content/drive/MyDrive/MobileNetV2/class_weights.csv', index=False)\n",
        "print(\"âœ“ Poids des classes sauvegardÃ©s\")\n",
        "\n",
        "# Sauvegarde des DataFrames\n",
        "train_df.to_csv('/content/drive/MyDrive/MobileNetV2/train_data.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/MobileNetV2/val_data.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/MobileNetV2/test_data.csv', index=False)\n",
        "print(\"âœ“ Datasets sauvegardÃ©s\")\n",
        "\n",
        "# 14. RÃ‰SUMÃ‰ FINAL\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RÃ‰SUMÃ‰ COMPLET - DIVISION 70/15/15\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. DISTRIBUTION INITIALE:\")\n",
        "for class_name in class_names:\n",
        "    print(f\"   {class_name}: {class_counts[class_name]:6d} images\")\n",
        "\n",
        "print(\"\\n2. CIBLES D'Ã‰QUILIBRAGE (avant division):\")\n",
        "for class_name in class_names:\n",
        "    print(f\"   {class_name}: {target_counts[class_name]:6d} images\")\n",
        "\n",
        "print(\"\\n3. DATASETS FINAUX (aprÃ¨s Ã©quilibrage + division 70/15/15):\")\n",
        "print(f\"   Train:      {len(train_df):6d} images ({len(train_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"   Validation: {len(val_df):6d} images ({len(val_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"   Test:       {len(test_df):6d} images ({len(test_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"   Total:      {len(train_df)+len(val_df)+len(test_df):6d} images\")\n",
        "\n",
        "print(\"\\n4. DISTRIBUTION PAR CLASSE DANS CHAQUE SET (70/15/15):\")\n",
        "print(\"\\n   TRAIN:\")\n",
        "train_counts = Counter(train_df['class'])\n",
        "for class_name in class_names:\n",
        "    count = train_counts.get(class_name, 0)\n",
        "    total_class = target_counts[class_name]\n",
        "    print(f\"     {class_name}: {count:6d} images ({count/total_class*100:.1f}% du total classe)\")\n",
        "\n",
        "print(\"\\n   VALIDATION:\")\n",
        "val_counts = Counter(val_df['class'])\n",
        "for class_name in class_names:\n",
        "    count = val_counts.get(class_name, 0)\n",
        "    total_class = target_counts[class_name]\n",
        "    print(f\"     {class_name}: {count:6d} images ({count/total_class*100:.1f}% du total classe)\")\n",
        "\n",
        "print(\"\\n   TEST:\")\n",
        "test_counts = Counter(test_df['class'])\n",
        "for class_name in class_names:\n",
        "    count = test_counts.get(class_name, 0)\n",
        "    total_class = target_counts[class_name]\n",
        "    print(f\"     {class_name}: {count:6d} images ({count/total_class*100:.1f}% du total classe)\")\n",
        "\n",
        "print(\"\\n5. POIDS DES CLASSES (calculÃ©s sur train):\")\n",
        "for idx, weight in class_weights.items():\n",
        "    print(f\"   {reverse_indices[idx]}: {weight:.4f}\")\n",
        "\n",
        "print(\"\\n6. RÃ‰SULTATS D'ENTRAÃŽNEMENT:\")\n",
        "print(f\"   Final Train Loss:      {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"   Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "print(f\"   Final Train Accuracy:  {history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"   Final Val Accuracy:    {history.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"   Final Train Precision: {history.history['precision'][-1]:.4f}\")\n",
        "print(f\"   Final Val Precision:   {history.history['val_precision'][-1]:.4f}\")\n",
        "print(f\"   Final Train Recall:    {history.history['recall'][-1]:.4f}\")\n",
        "print(f\"   Final Val Recall:      {history.history['val_recall'][-1]:.4f}\")\n",
        "\n",
        "print(\"\\n7. RÃ‰SULTATS SUR TEST SET:\")\n",
        "print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"   Test F1-Score (macro): {test_f1:.4f}\")\n",
        "\n",
        "print(\"\\n8. FICHIERS CRÃ‰Ã‰S:\")\n",
        "print(f\"   - final_age_classifier.h5 (modÃ¨le final)\")\n",
        "print(f\"   - best_model.h5 (meilleur modÃ¨le)\")\n",
        "print(f\"   - training_log.csv (historique)\")\n",
        "print(f\"   - class_indices.csv\")\n",
        "print(f\"   - class_weights.csv\")\n",
        "print(f\"   - train_data.csv\")\n",
        "print(f\"   - val_data.csv\")\n",
        "print(f\"   - test_data.csv\")\n",
        "print(f\"   - confusion_matrix_test.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENTRAÃŽNEMENT ET Ã‰VALUATION TERMINÃ‰S AVEC SUCCÃˆS!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 15. FONCTION DE PRÃ‰DICTION\n",
        "def predict_age(image_path):\n",
        "    \"\"\"PrÃ©dit la classe d'Ã¢ge d'une image\"\"\"\n",
        "    # Charger modÃ¨le\n",
        "    custom_objects = {'FocalLoss': FocalLoss}\n",
        "    loaded_model = keras.models.load_model('/content/drive/MyDrive/MobileNetV2/final_age_classifier.h5',\n",
        "                                          custom_objects=custom_objects)\n",
        "\n",
        "    # Charger indices\n",
        "    indices_df = pd.read_csv('/content/drive/MyDrive/MobileNetV2/class_indices.csv')\n",
        "    class_mapping = dict(zip(indices_df['class_index'], indices_df['class_name']))\n",
        "\n",
        "    # PrÃ©traitement\n",
        "    img = keras.preprocessing.image.load_img(image_path, target_size=IMAGE_SIZE)\n",
        "    img_array = keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "    # PrÃ©diction\n",
        "    predictions = loaded_model.predict(img_array, verbose=0)[0]\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    confidence = predictions[predicted_class]\n",
        "\n",
        "    result = {\n",
        "        'classe': class_mapping[predicted_class],\n",
        "        'confiance': float(confidence),\n",
        "        'probabilites': {class_mapping[i]: float(p) for i, p in enumerate(predictions)}\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# 16. TEST SUR QUELQUES IMAGES DU TEST SET\n",
        "print(\"\\n=== PRÃ‰DICTIONS SUR QUELQUES IMAGES DE TEST ===\")\n",
        "\n",
        "# SÃ©lectionner 2 images par classe du test set\n",
        "test_samples = []\n",
        "for class_name in class_names:\n",
        "    class_test_samples = test_df[test_df['class'] == class_name].sample(\n",
        "        min(2, len(test_df[test_df['class'] == class_name])),\n",
        "        random_state=SEED\n",
        "    )\n",
        "    test_samples.extend(class_test_samples.to_dict('records'))\n",
        "\n",
        "for i, sample in enumerate(test_samples):\n",
        "    print(f\"\\nTest {i+1}:\")\n",
        "    print(f\"  Image: {os.path.basename(sample['filename'])[:40]}...\")\n",
        "    print(f\"  Vraie classe: {sample['class']}\")\n",
        "\n",
        "    result = predict_age(sample['filename'])\n",
        "    print(f\"  PrÃ©diction: {result['classe']}\")\n",
        "    print(f\"  Confiance: {result['confiance']:.1%}\")\n",
        "\n",
        "    if result['classe'] == sample['class']:\n",
        "        print(\"  âœ“ Correct\")\n",
        "    else:\n",
        "        print(\"  âœ— Incorrect\")\n",
        "        print(f\"  ProbabilitÃ©s: {result['probabilites']}\")"
      ],
      "metadata": {
        "id": "MGKqYhBRnCuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS2RbkNWnAtu"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# TEST TIME AUGMENTATION (TTA) - AMÃ‰LIORATION IMMÃ‰DIATE +2-5% ACCURACY\n",
        "# Pas de rÃ©entraÃ®nement nÃ©cessaire !\n",
        "# ===============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸš€ TEST TIME AUGMENTATION (TTA)\")\n",
        "print(\"AmÃ©lioration immÃ©diate: +2-5% accuracy sans rÃ©entraÃ®ner!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "BASE_PATH = \"/content/drive/MyDrive/MobileNetV2\"\n",
        "IMAGE_SIZE = (224, 224)\n",
        "N_AUGMENTATIONS = 10  # Nombre d'augmentations par image (ajuster selon temps disponible)\n",
        "\n",
        "# ==================== FOCAL LOSS ====================\n",
        "class FocalLoss(keras.losses.Loss):\n",
        "    def __init__(self, gamma=2.0, alpha=0.25, name='focal_loss', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        weight = self.alpha * y_true * tf.pow((1 - y_pred), self.gamma)\n",
        "        focal_loss = weight * cross_entropy\n",
        "        focal_loss = tf.reduce_sum(focal_loss, axis=1)\n",
        "        return tf.reduce_mean(focal_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'gamma': self.gamma,\n",
        "            'alpha': self.alpha\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# ==================== CHARGEMENT ====================\n",
        "print(\"\\nðŸ“‚ CHARGEMENT DES DONNÃ‰ES ET MODÃˆLE...\")\n",
        "\n",
        "# Charger le test set\n",
        "test_df = pd.read_csv(f'{BASE_PATH}/test_data.csv')\n",
        "print(f\"âœ“ Test set: {len(test_df)} images\")\n",
        "\n",
        "# Distribution des classes\n",
        "class_counts = test_df['class'].value_counts()\n",
        "print(f\"\\nDistribution du test set:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"  {class_name}: {count} images ({count/len(test_df)*100:.1f}%)\")\n",
        "\n",
        "# Charger le meilleur modÃ¨le\n",
        "print(f\"\\nðŸ”„ Chargement du modÃ¨le...\")\n",
        "try:\n",
        "    model = keras.models.load_model(\n",
        "        f'{BASE_PATH}/best_model.h5',\n",
        "        custom_objects={'FocalLoss': FocalLoss}\n",
        "    )\n",
        "    print(\"âœ“ ModÃ¨le chargÃ©: best_model.h5\")\n",
        "except:\n",
        "    try:\n",
        "        model = keras.models.load_model(\n",
        "            f'{BASE_PATH}/final_age_classifier.h5',\n",
        "            custom_objects={'FocalLoss': FocalLoss}\n",
        "        )\n",
        "        print(\"âœ“ ModÃ¨le chargÃ©: final_age_classifier.h5\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur: {e}\")\n",
        "        raise\n",
        "\n",
        "# Classes\n",
        "class_names = sorted(test_df['class'].unique())\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
        "\n",
        "print(f\"âœ“ Classes dÃ©tectÃ©es: {class_names}\")\n",
        "\n",
        "# ==================== Ã‰VALUATION BASELINE ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š Ã‰VALUATION BASELINE (SANS TTA)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nÃ‰valuation du modÃ¨le original...\")\n",
        "\n",
        "# GÃ©nÃ©rateur simple sans augmentation\n",
        "baseline_datagen = ImageDataGenerator(rescale=1./255)\n",
        "baseline_generator = baseline_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# PrÃ©dictions baseline\n",
        "baseline_generator.reset()\n",
        "y_true_baseline = []\n",
        "y_pred_baseline = []\n",
        "\n",
        "for i in range(len(baseline_generator)):\n",
        "    x_batch, y_batch = next(baseline_generator)\n",
        "    batch_pred = model.predict(x_batch, verbose=0)\n",
        "\n",
        "    batch_true = np.argmax(y_batch, axis=1)\n",
        "    batch_pred_classes = np.argmax(batch_pred, axis=1)\n",
        "\n",
        "    y_true_baseline.extend(batch_true)\n",
        "    y_pred_baseline.extend(batch_pred_classes)\n",
        "\n",
        "# MÃ©triques baseline\n",
        "baseline_accuracy = np.mean(np.array(y_true_baseline) == np.array(y_pred_baseline))\n",
        "baseline_f1 = f1_score(y_true_baseline, y_pred_baseline, average='macro')\n",
        "\n",
        "print(f\"\\nðŸ“ˆ RÃ‰SULTATS BASELINE:\")\n",
        "print(f\"  Accuracy: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-Score (macro): {baseline_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ“‹ Rapport par classe (baseline):\")\n",
        "print(classification_report(\n",
        "    y_true_baseline,\n",
        "    y_pred_baseline,\n",
        "    target_names=class_names,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# ==================== TTA FUNCTIONS ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ”§ CONFIGURATION TTA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def create_tta_generator():\n",
        "    \"\"\"CrÃ©er le gÃ©nÃ©rateur pour TTA avec augmentations agressives\"\"\"\n",
        "    return ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,      # Rotation modÃ©rÃ©e\n",
        "        width_shift_range=0.15,  # DÃ©calage horizontal\n",
        "        height_shift_range=0.15, # DÃ©calage vertical\n",
        "        horizontal_flip=True,    # Flip horizontal\n",
        "        zoom_range=0.15,         # Zoom\n",
        "        brightness_range=[0.85, 1.15],  # Variation luminositÃ©\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "def predict_single_image_with_tta(model, image_path, n_augmentations=10):\n",
        "    \"\"\"\n",
        "    PrÃ©dire une seule image avec TTA\n",
        "\n",
        "    Args:\n",
        "        model: modÃ¨le Keras\n",
        "        image_path: chemin vers l'image\n",
        "        n_augmentations: nombre d'augmentations Ã  moyenner\n",
        "\n",
        "    Returns:\n",
        "        prediction: classe prÃ©dite\n",
        "        confidence: confiance de la prÃ©diction\n",
        "        all_probs: probabilitÃ©s moyennes pour toutes les classes\n",
        "    \"\"\"\n",
        "    # Charger l'image\n",
        "    img = keras.preprocessing.image.load_img(image_path, target_size=IMAGE_SIZE)\n",
        "    img_array = keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 1. PrÃ©diction normale (sans augmentation)\n",
        "    pred_normal = model.predict(img_array / 255.0, verbose=0)\n",
        "    predictions.append(pred_normal)\n",
        "\n",
        "    # 2. PrÃ©dictions avec augmentations\n",
        "    if n_augmentations > 1:\n",
        "        tta_gen = create_tta_generator()\n",
        "        augmented_gen = tta_gen.flow(img_array, batch_size=1, shuffle=False)\n",
        "\n",
        "        for _ in range(n_augmentations - 1):\n",
        "            img_aug = next(augmented_gen)\n",
        "            pred_aug = model.predict(img_aug, verbose=0)\n",
        "            predictions.append(pred_aug)\n",
        "\n",
        "    # 3. Moyenne des prÃ©dictions\n",
        "    avg_prediction = np.mean(predictions, axis=0)[0]\n",
        "\n",
        "    # 4. RÃ©sultats\n",
        "    predicted_class = np.argmax(avg_prediction)\n",
        "    confidence = avg_prediction[predicted_class]\n",
        "\n",
        "    return predicted_class, confidence, avg_prediction\n",
        "\n",
        "print(f\"\\nâœ“ Configuration TTA:\")\n",
        "print(f\"  Nombre d'augmentations par image: {N_AUGMENTATIONS}\")\n",
        "print(f\"  Types d'augmentation: rotation, shift, flip, zoom, brightness\")\n",
        "print(f\"  StratÃ©gie: moyenne des prÃ©dictions\")\n",
        "\n",
        "# ==================== Ã‰VALUATION AVEC TTA ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽ¯ Ã‰VALUATION AVEC TTA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nDÃ©but de l'Ã©valuation avec TTA sur {len(test_df)} images...\")\n",
        "print(\"â³ Cela peut prendre quelques minutes...\")\n",
        "\n",
        "y_true_tta = []\n",
        "y_pred_tta = []\n",
        "confidences_tta = []\n",
        "all_predictions_tta = []\n",
        "\n",
        "# Barre de progression\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"TTA Progress\"):\n",
        "    img_path = row['filename']\n",
        "    true_class = class_to_idx[row['class']]\n",
        "\n",
        "    # PrÃ©diction avec TTA\n",
        "    pred_class, confidence, all_probs = predict_single_image_with_tta(\n",
        "        model,\n",
        "        img_path,\n",
        "        n_augmentations=N_AUGMENTATIONS\n",
        "    )\n",
        "\n",
        "    y_true_tta.append(true_class)\n",
        "    y_pred_tta.append(pred_class)\n",
        "    confidences_tta.append(confidence)\n",
        "    all_predictions_tta.append(all_probs)\n",
        "\n",
        "# ==================== ANALYSE DES RÃ‰SULTATS ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š RÃ‰SULTATS FINAUX\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# MÃ©triques TTA\n",
        "tta_accuracy = np.mean(np.array(y_true_tta) == np.array(y_pred_tta))\n",
        "tta_f1 = f1_score(y_true_tta, y_pred_tta, average='macro')\n",
        "tta_f1_weighted = f1_score(y_true_tta, y_pred_tta, average='weighted')\n",
        "tta_f1_per_class = f1_score(y_true_tta, y_pred_tta, average=None)\n",
        "\n",
        "# Comparaison\n",
        "improvement = (tta_accuracy - baseline_accuracy) * 100\n",
        "improvement_f1 = (tta_f1 - baseline_f1) * 100\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ðŸŽ‰ COMPARAISON BASELINE vs TTA\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ ACCURACY:\")\n",
        "print(f\"  Baseline:  {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
        "print(f\"  Avec TTA:  {tta_accuracy:.4f} ({tta_accuracy*100:.2f}%)\")\n",
        "print(f\"  {'ðŸš€ GAIN:':12} {improvement:+.2f}%\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ F1-SCORE (MACRO):\")\n",
        "print(f\"  Baseline:  {baseline_f1:.4f}\")\n",
        "print(f\"  Avec TTA:  {tta_f1:.4f}\")\n",
        "print(f\"  {'ðŸš€ GAIN:':12} {improvement_f1:+.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ F1-SCORE (WEIGHTED):\")\n",
        "print(f\"  Avec TTA:  {tta_f1_weighted:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ F1-SCORE PAR CLASSE (TTA):\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    baseline_f1_class = f1_score(\n",
        "        [1 if y == i else 0 for y in y_true_baseline],\n",
        "        [1 if y == i else 0 for y in y_pred_baseline],\n",
        "        average='binary'\n",
        "    )\n",
        "    tta_f1_class = tta_f1_per_class[i]\n",
        "    improvement_class = (tta_f1_class - baseline_f1_class) * 100\n",
        "\n",
        "    print(f\"  {class_name:8} | Baseline: {baseline_f1_class:.4f} | TTA: {tta_f1_class:.4f} | Gain: {improvement_class:+.2f}%\")\n",
        "\n",
        "# Rapport dÃ©taillÃ©\n",
        "print(f\"\\nðŸ“‹ RAPPORT DE CLASSIFICATION DÃ‰TAILLÃ‰ (TTA):\")\n",
        "print(classification_report(\n",
        "    y_true_tta,\n",
        "    y_pred_tta,\n",
        "    target_names=class_names,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# ==================== MATRICE DE CONFUSION ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š MATRICES DE CONFUSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Baseline\n",
        "cm_baseline = confusion_matrix(y_true_baseline, y_pred_baseline)\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            ax=axes[0])\n",
        "axes[0].set_title(f'Matrice de Confusion - Baseline\\nAccuracy: {baseline_accuracy*100:.2f}%',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Vraie classe')\n",
        "axes[0].set_xlabel('Classe prÃ©dite')\n",
        "\n",
        "# TTA\n",
        "cm_tta = confusion_matrix(y_true_tta, y_pred_tta)\n",
        "sns.heatmap(cm_tta, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            ax=axes[1])\n",
        "axes[1].set_title(f'Matrice de Confusion - Avec TTA\\nAccuracy: {tta_accuracy*100:.2f}% (+{improvement:.2f}%)',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Vraie classe')\n",
        "axes[1].set_xlabel('Classe prÃ©dite')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{BASE_PATH}/confusion_matrix_tta_comparison.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"âœ“ Matrices sauvegardÃ©es: confusion_matrix_tta_comparison.png\")\n",
        "plt.show()\n",
        "\n",
        "# ==================== ANALYSE DES CONFIANCES ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ” ANALYSE DES CONFIANCES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "avg_confidence = np.mean(confidences_tta)\n",
        "print(f\"\\nConfiance moyenne des prÃ©dictions: {avg_confidence:.4f} ({avg_confidence*100:.2f}%)\")\n",
        "\n",
        "# Confiance par classe\n",
        "print(f\"\\nConfiance moyenne par classe:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_mask = np.array(y_pred_tta) == i\n",
        "    if np.sum(class_mask) > 0:\n",
        "        class_confidence = np.mean([confidences_tta[j] for j in range(len(confidences_tta)) if class_mask[j]])\n",
        "        print(f\"  {class_name}: {class_confidence:.4f} ({class_confidence*100:.2f}%)\")\n",
        "\n",
        "# Distribution des confiances\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(confidences_tta, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.axvline(avg_confidence, color='red', linestyle='--', linewidth=2, label=f'Moyenne: {avg_confidence:.2f}')\n",
        "plt.title('Distribution des Confiances (TTA)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Confiance')\n",
        "plt.ylabel('Nombre de prÃ©dictions')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Confiance par classe\n",
        "plt.subplot(1, 2, 2)\n",
        "confidence_by_class = []\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_mask = np.array(y_pred_tta) == i\n",
        "    class_confidences = [confidences_tta[j] for j in range(len(confidences_tta)) if class_mask[j]]\n",
        "    confidence_by_class.append(class_confidences)\n",
        "\n",
        "plt.boxplot(confidence_by_class, labels=class_names, patch_artist=True,\n",
        "            boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
        "plt.title('Confiances par Classe (TTA)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Confiance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{BASE_PATH}/confidence_analysis_tta.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"âœ“ Analyse des confiances sauvegardÃ©e: confidence_analysis_tta.png\")\n",
        "plt.show()\n",
        "\n",
        "# ==================== SAUVEGARDE DES RÃ‰SULTATS ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ’¾ SAUVEGARDE DES RÃ‰SULTATS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# CSV avec prÃ©dictions dÃ©taillÃ©es\n",
        "results_df = test_df.copy()\n",
        "results_df['true_class_idx'] = y_true_tta\n",
        "results_df['predicted_class'] = [idx_to_class[i] for i in y_pred_tta]\n",
        "results_df['confidence'] = confidences_tta\n",
        "results_df['correct'] = np.array(y_true_tta) == np.array(y_pred_tta)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    results_df[f'prob_{class_name}'] = [all_predictions_tta[j][i] for j in range(len(all_predictions_tta))]\n",
        "\n",
        "results_df.to_csv(f'{BASE_PATH}/test_predictions_tta.csv', index=False)\n",
        "print(f\"âœ“ PrÃ©dictions dÃ©taillÃ©es: test_predictions_tta.csv\")\n",
        "\n",
        "# JSON avec rÃ©sumÃ©\n",
        "summary = {\n",
        "    'baseline': {\n",
        "        'accuracy': float(baseline_accuracy),\n",
        "        'f1_macro': float(baseline_f1)\n",
        "    },\n",
        "    'tta': {\n",
        "        'accuracy': float(tta_accuracy),\n",
        "        'f1_macro': float(tta_f1),\n",
        "        'f1_weighted': float(tta_f1_weighted),\n",
        "        'n_augmentations': N_AUGMENTATIONS,\n",
        "        'avg_confidence': float(avg_confidence)\n",
        "    },\n",
        "    'improvement': {\n",
        "        'accuracy_gain': float(improvement),\n",
        "        'f1_gain': float(improvement_f1)\n",
        "    },\n",
        "    'per_class': {\n",
        "        class_names[i]: {\n",
        "            'f1_score': float(tta_f1_per_class[i]),\n",
        "            'precision': float(classification_report(y_true_tta, y_pred_tta, output_dict=True)[class_names[i]]['precision']),\n",
        "            'recall': float(classification_report(y_true_tta, y_pred_tta, output_dict=True)[class_names[i]]['recall'])\n",
        "        }\n",
        "        for i in range(len(class_names))\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{BASE_PATH}/tta_results_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=4)\n",
        "print(f\"âœ“ RÃ©sumÃ© JSON: tta_results_summary.json\")\n",
        "\n",
        "# ==================== ANALYSE DES ERREURS ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âŒ ANALYSE DES ERREURS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "errors_tta = np.where(np.array(y_true_tta) != np.array(y_pred_tta))[0]\n",
        "print(f\"\\nNombre total d'erreurs avec TTA: {len(errors_tta)}/{len(y_true_tta)} ({len(errors_tta)/len(y_true_tta)*100:.2f}%)\")\n",
        "\n",
        "# Erreurs corrigÃ©es par TTA\n",
        "errors_baseline_set = set(np.where(np.array(y_true_baseline) != np.array(y_pred_baseline))[0])\n",
        "errors_tta_set = set(errors_tta)\n",
        "corrected_errors = errors_baseline_set - errors_tta_set\n",
        "new_errors = errors_tta_set - errors_baseline_set\n",
        "\n",
        "print(f\"\\nðŸ”§ Erreurs CORRIGÃ‰ES par TTA: {len(corrected_errors)}\")\n",
        "print(f\"âš ï¸  Nouvelles erreurs avec TTA: {len(new_errors)}\")\n",
        "print(f\"âœ… Bilan net: {len(corrected_errors) - len(new_errors)} corrections\")\n",
        "\n",
        "# Taux d'erreur par classe\n",
        "print(f\"\\nðŸ“Š TAUX D'ERREUR PAR CLASSE:\")\n",
        "print(f\"{'Classe':<10} {'Baseline':<12} {'TTA':<12} {'AmÃ©lioration':<15}\")\n",
        "print(\"-\" * 50)\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_indices_baseline = [j for j in range(len(y_true_baseline)) if y_true_baseline[j] == i]\n",
        "    class_indices_tta = [j for j in range(len(y_true_tta)) if y_true_tta[j] == i]\n",
        "\n",
        "    errors_baseline_class = sum([1 for j in class_indices_baseline if y_true_baseline[j] != y_pred_baseline[j]])\n",
        "    errors_tta_class = sum([1 for j in class_indices_tta if y_true_tta[j] != y_pred_tta[j]])\n",
        "\n",
        "    error_rate_baseline = errors_baseline_class / len(class_indices_baseline) * 100 if len(class_indices_baseline) > 0 else 0\n",
        "    error_rate_tta = errors_tta_class / len(class_indices_tta) * 100 if len(class_indices_tta) > 0 else 0\n",
        "    improvement_error = error_rate_baseline - error_rate_tta\n",
        "\n",
        "    print(f\"{class_name:<10} {error_rate_baseline:>6.2f}%      {error_rate_tta:>6.2f}%      {improvement_error:>+6.2f}%\")\n",
        "\n",
        "# ==================== RECOMMANDATIONS ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ’¡ RECOMMANDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if improvement >= 2.0:\n",
        "    print(f\"\\nâœ… EXCELLENT! Gain de {improvement:.2f}% avec TTA\")\n",
        "    print(f\"   Votre modÃ¨le bÃ©nÃ©ficie bien de l'augmentation de donnÃ©es.\")\n",
        "elif improvement >= 1.0:\n",
        "    print(f\"\\nðŸ‘ BON! Gain de {improvement:.2f}% avec TTA\")\n",
        "    print(f\"   AmÃ©lioration notable mais potentiel pour plus.\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  GAIN LIMITÃ‰: {improvement:.2f}% avec TTA\")\n",
        "    print(f\"   Le modÃ¨le ne bÃ©nÃ©ficie pas beaucoup de TTA.\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ PROCHAINES Ã‰TAPES:\")\n",
        "\n",
        "if tta_accuracy < 0.80:\n",
        "    print(f\"   1. âš¡ URGENT: Essayer l'ensemble de modÃ¨les (gain +3-5%)\")\n",
        "    print(f\"   2. ðŸ”§ Affiner avec plus d'augmentations (N={N_AUGMENTATIONS*2})\")\n",
        "    print(f\"   3. ðŸŽ“ Fine-tuning ciblÃ© sur classe 21-50\")\n",
        "elif tta_accuracy < 0.85:\n",
        "    print(f\"   1. ðŸŽ¯ Combiner TTA + Ensemble pour viser 85%+\")\n",
        "    print(f\"   2. ðŸ” Analyser les erreurs restantes\")\n",
        "    print(f\"   3. ðŸ“Š Optionnel: Ajuster les seuils de dÃ©cision\")\n",
        "else:\n",
        "    print(f\"   1. ðŸŽ‰ Excellent rÃ©sultat! PrÃªt pour production\")\n",
        "    print(f\"   2. ðŸ“ˆ Optionnel: Ensemble pour optimisation finale\")\n",
        "    print(f\"   3. ðŸš€ DÃ©ploiement recommandÃ©\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ FICHIERS CRÃ‰Ã‰S:\")\n",
        "print(f\"   - confusion_matrix_tta_comparison.png\")\n",
        "print(f\"   - confidence_analysis_tta.png\")\n",
        "print(f\"   - test_predictions_tta.csv\")\n",
        "print(f\"   - tta_results_summary.json\")\n",
        "\n",
        "# ==================== RÃ‰SUMÃ‰ FINAL ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽ‰ RÃ‰SUMÃ‰ FINAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                    RÃ‰SULTATS TTA FINAUX                        â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘  Accuracy Baseline:     {baseline_accuracy*100:5.2f}%                               â•‘\n",
        "â•‘  Accuracy avec TTA:     {tta_accuracy*100:5.2f}%                               â•‘\n",
        "â•‘  ðŸš€ GAIN TOTAL:          {improvement:+5.2f}%                               â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘  F1-Score (macro):      {tta_f1:.4f}                              â•‘\n",
        "â•‘  F1-Score (weighted):   {tta_f1_weighted:.4f}                              â•‘\n",
        "â•‘  Confiance moyenne:     {avg_confidence*100:5.2f}%                               â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘  Erreurs corrigÃ©es:     {len(corrected_errors):>4} images                         â•‘\n",
        "â•‘  Nouvelles erreurs:     {len(new_errors):>4} images                         â•‘\n",
        "â•‘  Bilan net:             {len(corrected_errors) - len(new_errors):>+4} corrections                    â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n",
        "\n",
        "print(\"âœ… Ã‰VALUATION TTA TERMINÃ‰E AVEC SUCCÃˆS!\\n\")\n",
        "\n",
        "# ==================== FONCTION DE PRÃ‰DICTION POUR PRODUCTION ====================\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸ”§ FONCTION DE PRÃ‰DICTION POUR PRODUCTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def predict_with_tta_production(image_path, model, n_augmentations=N_AUGMENTATIONS):\n",
        "    \"\"\"\n",
        "    Fonction de prÃ©diction prÃªte pour production\n",
        "\n",
        "    Usage:\n",
        "        result = predict_with_tta_production('path/to/image.jpg', model)\n",
        "        print(f\"Classe: {result['classe']}\")\n",
        "        print(f\"Confiance: {result['confiance']:.2%}\")\n",
        "    \"\"\"\n",
        "    pred_class, confidence, all_probs = predict_single_image_with_tta(\n",
        "        model, image_path, n_augmentations\n",
        "    )\n",
        "\n",
        "    result = {\n",
        "        'classe': idx_to_class[pred_class],\n",
        "        'confiance': float(confidence),\n",
        "        'probabilites': {\n",
        "            class_names[i]: float(all_probs[i])\n",
        "            for i in range(len(class_names))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"\\nâœ“ Fonction de prÃ©diction prÃªte:\")\n",
        "print(\"\"\"\n",
        "# Exemple d'utilisation:\n",
        "result = predict_with_tta_production('chemin/vers/image.jpg', model)\n",
        "print(f\"PrÃ©diction: {result['classe']} (confiance: {result['confiance']:.2%})\")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽŠ TERMINÃ‰! Votre modÃ¨le est maintenant plus performant avec TTA!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ]
}