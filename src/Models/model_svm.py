"""
üé≠ SVM 100% CLASSIQUE - Classification d'√Çge (3 Classes)
M√©thodes classiques uniquement : HOG, LBP, Histogrammes
"""

import os
import cv2
import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import joblib
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ============================================================
#                      CONFIGURATION OPTIMIS√âE
# ============================================================
class SVMConfig:
    """Configuration optimis√©e pour SVM sur dataset augment√©"""
    
    # Chemins
    DATASET_PATH = Path("data\dataset_augmente_3classes")
    
    # LIMITATION CRUCIALE : √©chantillonnage pour √©viter surcharge
    MAX_SAMPLES_PER_CLASS = {
        'train': 1000,   # Maximum d'images par classe pour train
        'val': None,     # Prendre toutes les images val
        'test': None     # Prendre toutes les images test
    }
    
    # Extraction de caract√©ristiques CLASSIQUES uniquement
    FEATURE_TYPE = 'hog'  # Options: 'hog', 'lbp', 'histogram'
    # 'hog' = Recommand√© (meilleur compromis vitesse/performance)
    # 'lbp' = Plus rapide, moins pr√©cis
    # 'histogram' = Le plus rapide, basique
    IMG_SIZE = (128, 128)
    
    # PCA
    USE_PCA = True
    PCA_COMPONENTS = 100
    
    # SVM - Param√®tres restreints pour GridSearch rapide
    PARAM_GRID = {
        'C': [1, 10],
        'gamma': ['scale', 0.001],
        'kernel': ['rbf']
    }
    
    # Gestion du d√©s√©quilibre
    USE_CLASS_WEIGHT = True
    
    # Seed
    SEED = 42

np.random.seed(SVMConfig.SEED)

# ============================================================
#          EXTRACTION DE CARACT√âRISTIQUES
# ============================================================
def extract_hog_features(image):
    """Extrait HOG"""
    from skimage.feature import hog
    
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    gray = cv2.resize(gray, SVMConfig.IMG_SIZE)
    
    features = hog(gray,
                   orientations=9,
                   pixels_per_cell=(8, 8),
                   cells_per_block=(2, 2),
                   block_norm='L2-Hys',
                   transform_sqrt=True,
                   feature_vector=True)
    
    return features

def extract_lbp_features(image):
    """Extrait LBP"""
    from skimage.feature import local_binary_pattern
    
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    gray = cv2.resize(gray, SVMConfig.IMG_SIZE)
    
    radius = 3
    n_points = 8 * radius
    lbp = local_binary_pattern(gray, n_points, radius, 'uniform')
    
    n_bins = int(lbp.max() + 1)
    hist, _ = np.histogram(lbp, density=True, bins=n_bins, range=(0, n_bins))
    
    return hist

def extract_color_histogram(image):
    """Extrait histogramme couleur"""
    image_resized = cv2.resize(image, SVMConfig.IMG_SIZE)
    
    hist_b = cv2.calcHist([image_resized], [0], None, [32], [0, 256])
    hist_g = cv2.calcHist([image_resized], [1], None, [32], [0, 256])
    hist_r = cv2.calcHist([image_resized], [2], None, [32], [0, 256])
    
    hist_b = cv2.normalize(hist_b, hist_b).flatten()
    hist_g = cv2.normalize(hist_g, hist_g).flatten()
    hist_r = cv2.normalize(hist_r, hist_r).flatten()
    
    hist_features = np.hstack([hist_b, hist_g, hist_r])
    
    return hist_features

# ============================================================
#          CHARGEMENT AVEC √âCHANTILLONNAGE
# ============================================================
def load_dataset_sampled(split='train'):
    """
    Charge avec √©chantillonnage stratifi√© pour √©viter surcharge m√©moire
    """
    split_path = SVMConfig.DATASET_PATH / split
    max_samples = SVMConfig.MAX_SAMPLES_PER_CLASS.get(split)
    
    images = []
    labels = []
    
    print(f"\nüìÇ Chargement {split} (√©chantillonn√©)...")
    
    for class_name in sorted(os.listdir(split_path)):
        class_path = split_path / class_name
        
        if not class_path.is_dir():
            continue
        
        # Lister toutes les images
        image_files = list(class_path.glob('*.jpg')) + list(class_path.glob('*.png'))
        
        # √âCHANTILLONNAGE si n√©cessaire
        if max_samples and len(image_files) > max_samples:
            print(f"   ‚ö†Ô∏è  {class_name}: {len(image_files)} images ‚Üí √©chantillonn√© √† {max_samples}")
            image_files = np.random.choice(image_files, max_samples, replace=False)
        
        print(f"   üìÅ {class_name}: chargement de {len(image_files)} images...", end=" ")
        
        class_images = []
        for img_path in image_files:
            try:
                img = cv2.imread(str(img_path))
                if img is not None:
                    class_images.append(img)
            except Exception as e:
                continue
        
        images.extend(class_images)
        labels.extend([class_name] * len(class_images))
        
        print(f"‚úÖ {len(class_images)} charg√©es")
    
    print(f"üìä Total {split}: {len(images)} images")
    return images, labels

# ============================================================
#          EXTRACTION AVEC GESTION D'ERREURS
# ============================================================
def extract_features_safe(images, feature_type='hog'):
    """Extrait features CLASSIQUES avec gestion d'erreurs robuste"""
    print(f"\nüîç Extraction des caract√©ristiques ({feature_type})...")
    
    # V√©rifier que c'est bien une m√©thode classique
    if feature_type not in ['hog', 'lbp', 'histogram']:
        raise ValueError(f"M√©thode non classique: {feature_type}. Utilisez 'hog', 'lbp' ou 'histogram'")
    
    features_list = []
    failed = 0
    
    for i, img in enumerate(tqdm(images, desc=f"   {feature_type.upper()}")):
        try:
            if feature_type == 'hog':
                feat = extract_hog_features(img)
            elif feature_type == 'lbp':
                feat = extract_lbp_features(img)
            elif feature_type == 'histogram':
                feat = extract_color_histogram(img)
            
            features_list.append(feat)
            
        except Exception as e:
            failed += 1
            # Ajouter vecteur de z√©ros
            if features_list:
                features_list.append(np.zeros_like(features_list[0]))
            else:
                # Premi√®re image, on skip
                continue
    
    if failed > 0:
        print(f"   ‚ö†Ô∏è  {failed} images √©chou√©es (remplac√©es par z√©ros)")
    
    return np.array(features_list)

# ============================================================
#          ENTRA√éNEMENT SVM OPTIMIS√â
# ============================================================
def train_svm_optimized(X_train, y_train, X_val=None, y_val=None):
    """Entra√Æne SVM avec optimisations"""
    print("\nü§ñ Entra√Ænement SVM optimis√©...")
    
    # Encoder labels
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    
    if X_val is not None:
        y_val_encoded = le.transform(y_val)
    
    # Normalisation
    print("   üìè Normalisation...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    if X_val is not None:
        X_val_scaled = scaler.transform(X_val)
    
    # PCA si activ√©
    pca = None
    if SVMConfig.USE_PCA and X_train_scaled.shape[1] > SVMConfig.PCA_COMPONENTS:
        print(f"   üìâ PCA ({SVMConfig.PCA_COMPONENTS} composantes)...")
        pca = PCA(n_components=SVMConfig.PCA_COMPONENTS, random_state=SVMConfig.SEED)
        X_train_scaled = pca.fit_transform(X_train_scaled)
        
        variance_explained = pca.explained_variance_ratio_.sum()
        print(f"      Variance expliqu√©e: {variance_explained:.2%}")
        
        if X_val is not None:
            X_val_scaled = pca.transform(X_val_scaled)
    
    # Cr√©er SVM avec class_weight
    print("   üîç GridSearch avec validation crois√©e...")
    
    svm_base = SVC(
        random_state=SVMConfig.SEED,
        probability=True,
        class_weight='balanced' if SVMConfig.USE_CLASS_WEIGHT else None
    )
    
    # GridSearch RESTREINT
    grid_search = GridSearchCV(
        svm_base,
        SVMConfig.PARAM_GRID,
        cv=3,  # 3-fold au lieu de 5 pour aller plus vite
        n_jobs=-1,
        verbose=2,
        scoring='accuracy'
    )
    
    start_time = time.time()
    grid_search.fit(X_train_scaled, y_train_encoded)
    elapsed = time.time() - start_time
    
    print(f"\n   ‚úÖ GridSearch termin√© en {elapsed/60:.1f} minutes")
    print(f"   üéØ Meilleurs params: {grid_search.best_params_}")
    print(f"   üìä Score CV: {grid_search.best_score_:.4f}")
    
    # √âvaluation sur validation
    best_model = grid_search.best_estimator_
    
    if X_val is not None:
        val_pred = best_model.predict(X_val_scaled)
        val_acc = accuracy_score(y_val_encoded, val_pred)
        print(f"\n   üìà Accuracy validation: {val_acc:.4f}")
        
        print("\n   üìã Rapport validation:")
        print(classification_report(y_val_encoded, val_pred, 
                                   target_names=le.classes_, 
                                   digits=4))
    
    return {
        'model': best_model,
        'scaler': scaler,
        'pca': pca,
        'label_encoder': le,
        'best_params': grid_search.best_params_,
        'cv_score': grid_search.best_score_,
        'grid_results': pd.DataFrame(grid_search.cv_results_)
    }

# ============================================================
#          √âVALUATION
# ============================================================
def evaluate_model(model_dict, X_test, y_test):
    """√âvalue le mod√®le sur test"""
    print("\nüß™ √âvaluation sur le jeu de test...")
    
    model = model_dict['model']
    scaler = model_dict['scaler']
    pca = model_dict['pca']
    le = model_dict['label_encoder']
    
    # Pr√©parer test
    X_test_scaled = scaler.transform(X_test)
    if pca:
        X_test_scaled = pca.transform(X_test_scaled)
    
    y_test_encoded = le.transform(y_test)
    
    # Pr√©dictions
    test_pred = model.predict(X_test_scaled)
    test_proba = model.predict_proba(X_test_scaled)
    
    test_acc = accuracy_score(y_test_encoded, test_pred)
    
    print(f"\nüìä Accuracy test: {test_acc:.4f}")
    
    print("\nüìã Rapport de classification (test):")
    print(classification_report(y_test_encoded, test_pred,
                               target_names=le.classes_,
                               digits=4))
    
    # Matrice de confusion
    cm = confusion_matrix(y_test_encoded, test_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=le.classes_,
                yticklabels=le.classes_)
    plt.title(f'Matrice de Confusion - SVM ({SVMConfig.FEATURE_TYPE.upper()})')
    plt.ylabel('Vrai label')
    plt.xlabel('Pr√©diction')
    plt.tight_layout()
    plt.savefig('logs/confusion_matrix_svm_optimized.png', dpi=300)
    print("\nüíæ Matrice sauvegard√©e: confusion_matrix_svm_optimized.png")
    plt.show()
    
    # M√©triques par classe
    print("\nüìä M√©triques d√©taill√©es par classe:")
    for i, class_name in enumerate(le.classes_):
        class_acc = np.mean(test_pred[y_test_encoded == i] == i)
        print(f"   {class_name}: {class_acc:.4f}")
    
    return {
        'accuracy': test_acc,
        'predictions': test_pred,
        'probabilities': test_proba,
        'confusion_matrix': cm
    }

# ============================================================
#          FONCTION PRINCIPALE
# ============================================================
def main_svm_optimized():
    """Pipeline SVM optimis√©"""
    
    print("\n" + "="*70)
    print("ü§ñ SVM 100% CLASSIQUE - CLASSIFICATION D'√ÇGE (3 CLASSES)")
    print("="*70)
    print(f"üìÅ Dataset: {SVMConfig.DATASET_PATH}")
    print(f"üîç Features CLASSIQUES: {SVMConfig.FEATURE_TYPE.upper()}")
    print(f"üìä √âchantillonnage train: {SVMConfig.MAX_SAMPLES_PER_CLASS['train']} par classe")
    print(f"‚öñÔ∏è  Class weight: {'Activ√©' if SVMConfig.USE_CLASS_WEIGHT else 'D√©sactiv√©'}")
    print("="*70)
    
    # V√©rifier dataset
    if not SVMConfig.DATASET_PATH.exists():
        print(f"\n‚ùå Dataset introuvable: {SVMConfig.DATASET_PATH}")
        print("   Ex√©cutez d'abord le pipeline d'augmentation.")
        return False
    
    # 1. Charger avec √©chantillonnage
    print("\n" + "="*70)
    print("üì• √âTAPE 1: CHARGEMENT DES DONN√âES")
    print("="*70)
    
    X_train_img, y_train = load_dataset_sampled('train')
    X_val_img, y_val = load_dataset_sampled('val')
    X_test_img, y_test = load_dataset_sampled('test')
    
    # Distribution
    print("\nüìä Distribution:")
    for split, labels in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
        unique, counts = np.unique(labels, return_counts=True)
        print(f"   {split:5s}: {dict(zip(unique, counts))}")
    
    # 2. Extraire features
    print("\n" + "="*70)
    print("üîß √âTAPE 2: EXTRACTION DES CARACT√âRISTIQUES")
    print("="*70)
    
    X_train = extract_features_safe(X_train_img, SVMConfig.FEATURE_TYPE)
    X_val = extract_features_safe(X_val_img, SVMConfig.FEATURE_TYPE)
    X_test = extract_features_safe(X_test_img, SVMConfig.FEATURE_TYPE)
    
    print(f"\n‚úÖ Shapes:")
    print(f"   Train: {X_train.shape}")
    print(f"   Val:   {X_val.shape}")
    print(f"   Test:  {X_test.shape}")
    
    # 3. Entra√Æner
    print("\n" + "="*70)
    print("üöÄ √âTAPE 3: ENTRA√éNEMENT SVM")
    print("="*70)
    
    model_dict = train_svm_optimized(X_train, y_train, X_val, y_val)
    
    # 4. √âvaluer
    print("\n" + "="*70)
    print("üß™ √âTAPE 4: √âVALUATION FINALE")
    print("="*70)
    
    results = evaluate_model(model_dict, X_test, y_test)
    
    # 5. Sauvegarder
    print("\n" + "="*70)
    print("üíæ √âTAPE 5: SAUVEGARDE")
    print("="*70)
    
    save_path = Path("models")
    save_path.mkdir(exist_ok=True)
    
    joblib.dump(model_dict['model'], save_path / 'svm_model.pkl')
    joblib.dump(model_dict['scaler'], save_path / 'scaler.pkl')
    if model_dict['pca']:
        joblib.dump(model_dict['pca'], save_path / 'pca.pkl')
    joblib.dump(model_dict['label_encoder'], save_path / 'label_encoder.pkl')
    
    # Config
    import json
    config_save = {
        'feature_type': SVMConfig.FEATURE_TYPE,
        'img_size': SVMConfig.IMG_SIZE,
        'max_samples_per_class': SVMConfig.MAX_SAMPLES_PER_CLASS,
        'use_pca': SVMConfig.USE_PCA,
        'pca_components': SVMConfig.PCA_COMPONENTS,
        'use_class_weight': SVMConfig.USE_CLASS_WEIGHT,
        'best_params': model_dict['best_params'],
        'cv_score': float(model_dict['cv_score']),
        'test_accuracy': float(results['accuracy']),
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open(save_path / 'config.json', 'w') as f:
        json.dump(config_save, f, indent=2)
    
    print(f"   ‚úÖ Mod√®le sauvegard√©: {save_path}")
    
    # 6. Rapport final
    print("\n" + "="*70)
    print("üìà RAPPORT FINAL")
    print("="*70)
    print(f"üéØ Features: {SVMConfig.FEATURE_TYPE}")
    print(f"üîß Params: {model_dict['best_params']}")
    print(f"üìä CV Score: {model_dict['cv_score']:.4f}")
    print(f"üß™ Test Accuracy: {results['accuracy']:.4f}")
    print("="*70)
    
    print("\n‚úÖ Pipeline SVM termin√© avec succ√®s!")
    
    return True

# ============================================================
#          POINT D'ENTR√âE
# ============================================================
if __name__ == "__main__":
    # V√©rifier d√©pendances
    try:
        import cv2
        import sklearn
        import matplotlib
        from skimage.feature import hog
        print("‚úÖ D√©pendances install√©es\n")
    except ImportError as e:
        print(f"‚ùå D√©pendance manquante: {e}")
        print("\nüì¶ Installation:")
        print("pip install scikit-learn opencv-python matplotlib seaborn joblib scikit-image tqdm")
        exit(1)
    
    success = main_svm_optimized()
    
    if not success:
        exit(1)